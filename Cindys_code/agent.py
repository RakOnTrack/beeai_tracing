# backend/agent.py
# This file implements a RAG (Retrieval-Augmented Generation) system for company FAQs
# using the BeeAI framework, ChromaDB for vector storage, and OpenAI for LLM responses.

import asyncio
import os
import time
# Disable tokenizer parallelism to avoid warnings and potential conflicts
os.environ["TOKENIZERS_PARALLELISM"] = "false"
from typing import List

# OpenTelemetry imports for observability
try:
    from opentelemetry import trace
    from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
    from opentelemetry.sdk.trace import TracerProvider
    from opentelemetry.sdk.trace.export import BatchSpanProcessor
    from opentelemetry.sdk.resources import Resource
    OTEL_AVAILABLE = True
    print("‚úÖ OpenTelemetry available for observability")
except ImportError:
    OTEL_AVAILABLE = False
    print("‚ö†Ô∏è  OpenTelemetry not available. Install with: pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http")

# Import BeeAI framework components for building the agent system
from beeai_framework.backend.chat import ChatModel  # For LLM integration (OpenAI)
from beeai_framework.tools.tool import Tool  # Base class for creating tools
from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput  # Workflow management
from beeai_framework.emitter.emitter import Emitter  # Event emission system
from pydantic import BaseModel, Field  # Data validation and schema definition

# Import external libraries for RAG functionality
from sentence_transformers import SentenceTransformer  # For creating text embeddings
import chromadb  # Vector database for storing and searching FAQ embeddings

# Configuration constants for the RAG system
EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'  # Lightweight but effective embedding model
CHROMA_PERSIST_PATH = "./my_chroma_db"  # Local path for ChromaDB storage
CHROMA_COLLECTION_NAME = "company_faqs"  # Collection name in ChromaDB

# Global variables to store initialized components (singleton pattern)
# These are initialized once and reused across multiple requests
_embedding_model = None  # SentenceTransformer model for creating embeddings
_chroma_collection = None  # ChromaDB collection for FAQ storage
_llm = None  # OpenAI chat model instance
_faq_tool_instance = None  # FAQTool instance for searching FAQs
_agent_workflow = None  # AgentWorkflow for orchestrating the FAQ agent
_tracer_provider = None  # OpenTelemetry tracer provider for observability
_tracer = None  # OpenTelemetry tracer for creating spans

def setup_splunk_otel():
    """Set up OpenTelemetry for Splunk SignalFX observability."""
    if not OTEL_AVAILABLE:
        print("‚ö†Ô∏è  OpenTelemetry not available - skipping OTEL setup")
        return None
    
    try:
        # Configuration from environment variables
        OTEL_ENDPOINT = os.getenv("OTEL_ENDPOINT", "http://localhost:4328")
        SERVICE_NAME = os.getenv("SERVICE_NAME", "beeai-faq-agent")
        ENVIRONMENT = os.getenv("ENVIRONMENT", "production")
        
        print(f"üîß Setting up Splunk SignalFX OTEL integration...")
        print(f"   Endpoint: {OTEL_ENDPOINT}")
        print(f"   Service: {SERVICE_NAME}")
        print(f"   Environment: {ENVIRONMENT}")
        
        # Create resource with service information
        resource = Resource.create({
            "service.name": SERVICE_NAME,
            "service.version": "1.0.0",
            "deployment.environment": ENVIRONMENT,
            "telemetry.sdk.name": "beeai-framework",
            "telemetry.sdk.version": "0.1.17"
        })
        
        # Create tracer provider
        tracer_provider = TracerProvider(resource=resource)
        
        # Create OTLP exporter for Splunk SignalFX
        otlp_exporter = OTLPSpanExporter(
            endpoint=OTEL_ENDPOINT,
            headers={
                # Add any required headers for your Splunk setup
                # "Authorization": "Bearer your-token",
                # "X-SF-TOKEN": "your-signalfx-token"
            }
        )
        
        # Add batch processor
        tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
        
        # Set as global tracer provider
        trace.set_tracer_provider(tracer_provider)
        
        print("‚úÖ Splunk SignalFX OTEL integration configured successfully")
        return tracer_provider
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to configure Splunk SignalFX OTEL: {e}")
        print("üí° Check your OTEL endpoint and configuration")
        return None

def test_span_export(tracer_provider, endpoint: str):
    """Test that spans can be exported successfully to verify connectivity."""
    try:
        print(f"üß™ Testing span export to {endpoint}...")
        
        # Create a test span
        test_tracer = trace.get_tracer("span-test")
        with test_tracer.start_as_current_span("test_span") as span:
            span.set_attribute("test.attribute", "span_export_test")
            span.set_attribute("test.timestamp", time.time())
            span.set_attribute("test.service", "beeai-faq-agent")
        
        # Force export by shutting down the tracer provider
        tracer_provider.force_flush()
        
        print("‚úÖ Span export test completed successfully")
        print("üí° Check your Splunk dashboard for the test span")
        return True
        
    except Exception as e:
        print(f"‚ùå Span export test failed: {e}")
        print("üí° Check your OTEL endpoint connectivity")
        return False

def _setup_rag_system():
    """
    Initializes all RAG system components and stores them globally.
    
    This function implements a singleton pattern - it only initializes components
    once and reuses them for subsequent requests, improving performance.
    
    Returns:
        bool: True if setup successful, False otherwise
    """
    global _embedding_model, _chroma_collection, _llm, _faq_tool_instance, _agent_workflow, _tracer_provider, _tracer

    # Check if all components are already initialized
    if _embedding_model and _chroma_collection and _llm and _faq_tool_instance and _agent_workflow:
        print("RAG system already set up.")
        return True

    print("Setting up RAG system components...")
    
    # Step 0: Initialize OpenTelemetry for observability
    if OTEL_AVAILABLE:
        try:
            _tracer_provider = setup_splunk_otel()
            if _tracer_provider:
                _tracer = trace.get_tracer("beeai-faq-agent")
                print("‚úÖ OpenTelemetry tracer initialized for observability")
                
                # Test span export to verify connectivity
                otel_endpoint = os.getenv("OTEL_ENDPOINT", "http://localhost:4328")
                span_test_success = test_span_export(_tracer_provider, otel_endpoint)
                if not span_test_success:
                    print("‚ö†Ô∏è  Warning: Span export test failed - traces may not be reaching Splunk")
                else:
                    print("‚úÖ Span export test passed - traces are being sent to Splunk successfully")
            else:
                print("‚ö†Ô∏è  OpenTelemetry setup failed - continuing without observability")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error setting up OpenTelemetry: {e}")
            print("üí° Continuing without observability")
    else:
        print("‚ö†Ô∏è  OpenTelemetry not available - running without observability")
    
    # Step 1: Initialize the embedding model for converting text to vectors
    try:
        _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print("Embedding model loaded.")
    except Exception as e:
        print(f"Error loading embedding model: {e}")
        return False

    # Step 2: Initialize ChromaDB for vector storage and retrieval
    try:
        chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_PATH)
        _chroma_collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)
        print(f"ChromaDB collection '{CHROMA_COLLECTION_NAME}' ready with {_chroma_collection.count()} documents.")
    except Exception as e:
        print(f"Error connecting to ChromaDB: {e}")
        return False

    # Step 3: Initialize the OpenAI LLM for generating responses
    try:
        _llm = ChatModel.from_name(os.environ.get("OPENAI_MODEL", "openai:gpt-4o"))
        print("OpenAI LLM initialized.")
    except Exception as e:
        print(f"Error initializing LLM: {e}")
        return False

    # Step 4: Create the FAQ tool instance that combines embedding and search
    _faq_tool_instance = FAQTool(embedding_model=_embedding_model, chroma_collection=_chroma_collection)
    print("FAQTool instance created.")

    # Step 5: Create the agent workflow that orchestrates the FAQ answering process
    _agent_workflow = AgentWorkflow(name="Company FAQ Assistant")
    _agent_workflow.add_agent(
        name="FAQAgent",
        role="An expert in company FAQs.",
        instructions=(
            "You are an expert in company FAQs. Your primary goal is to answer questions based on the provided company FAQ information. "
            "If company FAQ information is provided in the input, prioritize using it to answer the user's question. "
            "If no relevant company FAQ information is provided or found, state that you cannot find the answer in the company FAQs. "
            "Do NOT try to use the 'faq_lookup_tool' on your own if context is already provided, as the information has already been retrieved for you."
        ),
        tools=[_faq_tool_instance],  # Give the agent access to the FAQ search tool
        llm=_llm,  # Provide the LLM for generating responses
    )
    print("Agent workflow created and agent added.")
    return True

class FAQTool(Tool):
    """
    A custom tool that extends the BeeAI Tool class to provide FAQ search functionality.
    
    This tool uses semantic search to find relevant FAQ information by:
    1. Converting user queries to embeddings
    2. Searching the ChromaDB collection for similar FAQ embeddings
    3. Returning the most relevant FAQ content
    """
    
    # Tool metadata for the BeeAI framework
    name: str = "faq_lookup_tool"
    description: str = "Searches the company's frequently asked questions for relevant answers using semantic search. Use this tool when the user asks a question about company policies, products, or general FAQs. Input should be a question string."

    class FAQToolInput(BaseModel):
        """
        Input schema for the FAQ tool, defining the expected input structure.
        
        This ensures that the tool receives properly formatted input data.
        """
        query: str = Field(description="The question to lookup in the company FAQs.")

    @property
    def input_schema(self) -> type[BaseModel]:
        """Returns the input schema class for validation."""
        return self.FAQToolInput

    def _create_emitter(self) -> Emitter | None:
        """Creates an emitter for tracking tool execution events."""
        return Emitter()

    def __init__(self, embedding_model: SentenceTransformer, chroma_collection: chromadb.Collection):
        """
        Initialize the FAQ tool with required components.
        
        Args:
            embedding_model: Model for converting text to embeddings
            chroma_collection: ChromaDB collection containing FAQ data
        """
        super().__init__()
        self.embedding_model = embedding_model
        self.chroma_collection = chroma_collection

    async def _run(self, query: str) -> str:
        """
        Main execution method for the FAQ tool.
        
        This method performs semantic search by:
        1. Converting the query to embeddings
        2. Searching ChromaDB for similar FAQ content
        3. Formatting and returning the results
        
        Args:
            query: The user's question to search for
            
        Returns:
            str: Formatted FAQ information or error message
        """
        # Start OpenTelemetry span for FAQ tool execution
        if _tracer:
            with _tracer.start_as_current_span("faq_tool_execution") as span:
                span.set_attribute("tool.name", self.name)
                span.set_attribute("tool.description", self.description)
                span.set_attribute("faq.query", query)
                span.set_attribute("faq.query_length", len(query))
                
                try:
                    # Step 1: Convert the user query to embeddings for semantic search
                    with _tracer.start_as_current_span("query_embedding") as embedding_span:
                        embedding_span.set_attribute("embedding.model", "sentence-transformers")
                        embedding_span.set_attribute("embedding.model_name", "all-MiniLM-L6-v2")
                        
                        query_embedding = self.embedding_model.encode(query).tolist()
                        embedding_span.set_attribute("embedding.vector_size", len(query_embedding))
                        embedding_span.set_attribute("embedding.success", True)
                    
                    # Step 2: Search ChromaDB for similar FAQ content using vector similarity
                    with _tracer.start_as_current_span("chroma_search") as search_span:
                        search_span.set_attribute("chroma.collection", CHROMA_COLLECTION_NAME)
                        search_span.set_attribute("chroma.n_results", 3)
                        
                        results = self.chroma_collection.query(
                            query_embeddings=[query_embedding],  # Search using the query embedding
                            n_results=3,  # Return top 3 most similar results
                            include=['documents', 'metadatas']  # Include both content and metadata
                        )
                        
                        search_span.set_attribute("chroma.results_count", len(results.get('documents', [[]])[0]) if results and results.get('documents') else 0)
                        search_span.set_attribute("chroma.search_success", True)
                    
                    # Step 3: Process and format the search results
                    with _tracer.start_as_current_span("result_processing") as process_span:
                        retrieved_contexts = []
                        if results and results.get('documents') and results['documents'][0]:
                            # Iterate through the retrieved documents and format them
                            for i in range(len(results['documents'][0])):
                                doc_content = results['documents'][0][i]  # The FAQ content
                                metadata = results['metadatas'][0][i]     # Associated metadata
                                question = metadata.get('question', 'N/A')  # Extract question from metadata
                                answer = metadata.get('answer', doc_content)  # Extract answer or use content
                                retrieved_contexts.append(f"Question: {question}\nAnswer: {answer}")
                        
                        process_span.set_attribute("processing.contexts_count", len(retrieved_contexts))
                        process_span.set_attribute("processing.success", True)
                    
                    # Step 4: Handle case where no relevant information is found
                    if not retrieved_contexts:
                        span.set_attribute("faq.no_results", True)
                        span.set_attribute("faq.status", "no_results")
                        return "No relevant information found in the FAQs."
                    
                    # Step 5: Combine all retrieved contexts into a single formatted string
                    context_string = "\n\n".join(retrieved_contexts)
                    span.set_attribute("faq.results_found", True)
                    span.set_attribute("faq.status", "success")
                    span.set_attribute("faq.response_length", len(context_string))
                    
                    return context_string
                    
                except Exception as e:
                    # Set span attributes for error
                    span.set_attribute("faq.status", "error")
                    span.set_attribute("error.message", str(e))
                    span.set_attribute("error.type", type(e).__name__)
                    
                    # Log error for observability
                    print(f"Error in FAQ tool execution: {e}")
                    return f"Error processing query for FAQ lookup: {e}"
        else:
            # Fallback without OpenTelemetry
            try:
                # Step 1: Convert the user query to embeddings for semantic search
                query_embedding = self.embedding_model.encode(query).tolist()
                
                # Step 2: Search ChromaDB for similar FAQ content using vector similarity
                results = self.chroma_collection.query(
                    query_embeddings=[query_embedding],  # Search using the query embedding
                    n_results=3,  # Return top 3 most similar results
                    include=['documents', 'metadatas']  # Include both content and metadata
                )
                
                # Step 3: Process and format the search results
                retrieved_contexts = []
                if results and results.get('documents') and results['documents'][0]:
                    # Iterate through the retrieved documents and format them
                    for i in range(len(results['documents'][0])):
                        doc_content = results['documents'][0][i]  # The FAQ content
                        metadata = results['metadatas'][0][i]     # Associated metadata
                        question = metadata.get('question', 'N/A')  # Extract question from metadata
                        answer = metadata.get('answer', doc_content)  # Extract answer or use content
                        retrieved_contexts.append(f"Question: {question}\nAnswer: {answer}")
                
                # Step 4: Handle case where no relevant information is found
                if not retrieved_contexts:
                    return "No relevant information found in the FAQs."
                
                # Step 5: Combine all retrieved contexts into a single formatted string
                context_string = "\n\n".join(retrieved_contexts)
                return context_string
                
            except Exception as e:
                return f"Error processing query for FAQ lookup: {e}"

async def run_faq_agent(user_query: str) -> str:
    """
    Main function to run the FAQ agent workflow.
    
    This function orchestrates the entire FAQ answering process:
    1. Ensures the RAG system is properly initialized
    2. Searches for relevant FAQ information
    3. Generates a comprehensive answer using the LLM
    
    Args:
        user_query: The user's question about company FAQs
        
    Returns:
        str: The agent's response to the user's question
    """
    # Start OpenTelemetry span for the main FAQ agent workflow
    if _tracer:
        with _tracer.start_as_current_span("faq_agent_workflow") as span:
            span.set_attribute("workflow.name", "Company FAQ Assistant")
            span.set_attribute("workflow.query", user_query)
            span.set_attribute("workflow.query_length", len(user_query))
            span.set_attribute("workflow.timestamp", time.time())
            
            try:
                # Ensure the RAG system is set up before processing
                with _tracer.start_as_current_span("rag_system_setup") as setup_span:
                    if not _setup_rag_system():
                        setup_span.set_attribute("setup.status", "failed")
                        span.set_attribute("workflow.status", "setup_failed")
                        return "Backend RAG system failed to initialize. Please check server logs."
                    setup_span.set_attribute("setup.status", "success")
                
                # Step 1: Manually call the FAQ tool to retrieve relevant information
                print("Calling the faq_lookup_tool...")
                with _tracer.start_as_current_span("faq_tool_execution") as tool_span:
                    tool_span.set_attribute("tool.name", "faq_lookup_tool")
                    retrieved_info = await _faq_tool_instance._run(user_query)
                    tool_span.set_attribute("tool.response_length", len(retrieved_info))
                    tool_span.set_attribute("tool.success", True)
               
                # Step 2: Build a comprehensive prompt combining retrieved info and user question
                # This gives the LLM context about relevant FAQs before asking it to answer
                prompt_for_agent = f"Retrieved Company FAQ Information:\n{retrieved_info}\n\nUser Question: {user_query}"
                
                with _tracer.start_as_current_span("prompt_construction") as prompt_span:
                    prompt_span.set_attribute("prompt.retrieved_info_length", len(retrieved_info))
                    prompt_span.set_attribute("prompt.final_length", len(prompt_for_agent))
                    prompt_span.set_attribute("prompt.construction_success", True)

                # Step 3: Run the agent workflow to generate the final answer
                with _tracer.start_as_current_span("agent_workflow_execution") as workflow_span:
                    workflow_span.set_attribute("workflow.agent_name", "FAQAgent")
                    workflow_span.set_attribute("workflow.llm_model", "openai:gpt-4o")
                    
                    response = await _agent_workflow.run(
                        inputs=[
                            AgentWorkflowInput(
                                prompt=prompt_for_agent,  # Send the combined prompt to the agent
                            )
                        ]
                    )
                    
                    final_answer = response.result.final_answer
                    workflow_span.set_attribute("workflow.response_length", len(final_answer))
                    workflow_span.set_attribute("workflow.success", True)
                    span.set_attribute("workflow.status", "success")
                    
                    return final_answer  # Extract the agent's response
                    
            except Exception as e:
                # Set span attributes for error
                span.set_attribute("workflow.status", "error")
                span.set_attribute("error.message", str(e))
                span.set_attribute("error.type", type(e).__name__)
                
                print(f"Error running agent workflow: {e}")
                return f"An error occurred while processing your request: {e}"
    else:
        # Fallback without OpenTelemetry
        # Ensure the RAG system is set up before processing
        if not _setup_rag_system():
            return "Backend RAG system failed to initialize. Please check server logs."

        # Step 1: Manually call the FAQ tool to retrieve relevant information
        print("Calling the faq_lookup_tool...")
        retrieved_info = await _faq_tool_instance._run(user_query)
       
        # Step 2: Build a comprehensive prompt combining retrieved info and user question
        # This gives the LLM context about relevant FAQs before asking it to answer
        prompt_for_agent = f"Retrieved Company FAQ Information:\n{retrieved_info}\n\nUser Question: {user_query}"

        # Step 3: Run the agent workflow to generate the final answer
        try:
            response = await _agent_workflow.run(
                inputs=[
                    AgentWorkflowInput(
                        prompt=prompt_for_agent,  # Send the combined prompt to the agent
                    )
                ]
            )
            return response.result.final_answer  # Extract the agent's response
        except Exception as e:
            print(f"Error running agent workflow: {e}")
            return f"An error occurred while processing your request: {e}"

def get_observability_data() -> dict:
    """
    Get observability data for monitoring the RAG system.
    
    Returns:
        dict: Dictionary containing observability metrics and status
    """
    return {
        "rag_system": {
            "embedding_model_loaded": _embedding_model is not None,
            "chroma_collection_ready": _chroma_collection is not None,
            "llm_initialized": _llm is not None,
            "faq_tool_ready": _faq_tool_instance is not None,
            "agent_workflow_ready": _agent_workflow is not None
        },
        "opentelemetry": {
            "available": OTEL_AVAILABLE,
            "tracer_provider_ready": _tracer_provider is not None,
            "tracer_ready": _tracer is not None
        },
        "chroma_db": {
            "collection_name": CHROMA_COLLECTION_NAME,
            "document_count": _chroma_collection.count() if _chroma_collection else 0,
            "persist_path": CHROMA_PERSIST_PATH
        },
        "embedding_model": {
            "name": EMBEDDING_MODEL_NAME,
            "loaded": _embedding_model is not None
        },
        "status": "ready" if all([
            _embedding_model, _chroma_collection, _llm, 
            _faq_tool_instance, _agent_workflow
        ]) else "initializing"
    }